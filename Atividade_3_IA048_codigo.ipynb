{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fz7yFvKXeGr"
      },
      "source": [
        "# Atividade 03 - Aprendizado de Máquina (IA048)\n",
        "\n",
        "Álvaro Tona Ribas Cruz - 239520\n",
        "\n",
        "Antonio César de Andrade Júnior - 245628"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAoS-rVfX39R"
      },
      "source": [
        "## 0 - Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htmYftbnp6Cm"
      },
      "outputs": [],
      "source": [
        "pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6Hla1gFgP6c"
      },
      "outputs": [],
      "source": [
        "pip install skorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYNcC1G5rTdU"
      },
      "outputs": [],
      "source": [
        "from medmnist import BloodMNIST\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from skorch import NeuralNetClassifier\n",
        "from skorch.callbacks import EarlyStopping\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, GlobalAveragePooling2D, Dense, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import LabelBinarizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99RhIrCWX70Z"
      },
      "source": [
        "## (a) MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk9Cf5IWi54t"
      },
      "source": [
        "CROSSVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hxkj36MLiVRm"
      },
      "outputs": [],
      "source": [
        "train = BloodMNIST(split=\"train\", download=True, size=28)\n",
        "test = BloodMNIST(split=\"test\", download=True, size=28)\n",
        "\n",
        "x_train = train.imgs.reshape(-1, 3*28*28)\n",
        "x_test = test.imgs.reshape(-1, 3*28*28)\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(train.labels.ravel())\n",
        "y_test = lb.transform(test.labels.ravel())\n",
        "\n",
        "\n",
        "def create_model(hidden_layer_size=64, activation='relu'):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Dense(hidden_layer_size, activation=activation, input_dim=x_train.shape[1]),\n",
        "        Dense(8, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=300, batch_size=32, verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "hidden_layer_size= [64 ,128, 256]\n",
        "activation= ['relu', 'tanh', 'sigmoid']\n",
        "\n",
        "param = dict(model__hidden_layer_size = hidden_layer_size, model__activation=activation)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param, cv=3)\n",
        "result = grid.fit(x_train, y_train)\n",
        "\n",
        "print(\"Melhores Parâmetros: %s - Score: %.4f\" % (result.best_params_, result.best_score_))\n",
        "\n",
        "print(\"Todos os resultados:\")\n",
        "for mean, std, params in zip(grid.cv_results_['mean_test_score'], grid.cv_results_['std_test_score'], grid.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) para %r\" % (mean, std * 2, params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xePP-TQUi9B-"
      },
      "source": [
        "Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPYXDj2Gie45"
      },
      "outputs": [],
      "source": [
        "train = BloodMNIST(split=\"train\", download= True, size = 28)\n",
        "\n",
        "val = BloodMNIST(split=\"val\", download= True, size = 28)\n",
        "\n",
        "test = BloodMNIST(split=\"test\", download= True, size = 28)\n",
        "\n",
        "x_train = train.imgs.reshape(-1, 3*28*28)\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(train.labels.ravel())\n",
        "\n",
        "\n",
        "x_val = val.imgs.reshape(-1, 3*28*28)\n",
        "lb = LabelBinarizer()\n",
        "y_val = lb.fit_transform(val.labels.ravel())\n",
        "\n",
        "mlp = tf.keras.models.Sequential()\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    min_delta=0.001,\n",
        "    patience=25,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0,\n",
        ")\n",
        "\n",
        "mlp.add(Dense(256, activation='relu'))\n",
        "mlp.add(Dense(8, activation='softmax'))\n",
        "\n",
        "mlp.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "mlp.fit(x_train, y_train, epochs=300, batch_size=32, callbacks=[callback],verbose =0, validation_data=(x_val, y_val))\n",
        "\n",
        "x_test = test.imgs.reshape(-1, 3*28*28)\n",
        "lb = LabelBinarizer()\n",
        "y_test = lb.fit_transform(test.labels.ravel())\n",
        "\n",
        "y_pred = mlp.predict(x_test)\n",
        "y_pred_indices = np.argmax(y_pred, axis=1)\n",
        "y_pred_one_hot = np.eye(np.max(y_pred_indices) + 1)[y_pred_indices]\n",
        "\n",
        "y_pred_indices\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred_one_hot)\n",
        "\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG1CZakpYMxt"
      },
      "source": [
        "## (b) CNN simples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpPrH2XtfysD"
      },
      "source": [
        "Rede Neural Convolucional.\n",
        "\n",
        "Ela contém as camadas pedidas no exercício, e as variáveis que devem ser controladas para avaliação do desempenho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjEvCcTzjF2z"
      },
      "outputs": [],
      "source": [
        "train = BloodMNIST(split=\"train\", download= True, size = 28)\n",
        "\n",
        "val = BloodMNIST(split=\"val\", download= True, size = 28)\n",
        "\n",
        "test = BloodMNIST(split=\"test\", download= True, size = 28)\n",
        "\n",
        "x_train = train.imgs / 255\n",
        "x_val = val.imgs / 255\n",
        "x_test = test.imgs / 255\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(train.labels.ravel())\n",
        "y_val = lb.fit_transform(val.labels.ravel())\n",
        "y_test = lb.transform(test.labels.ravel())\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    kernel_size = hp.Choice('kernel_size', ['2,2', '4,4', '8,8', '16,16'])\n",
        "    kernel_size = tuple(map(int, kernel_size.split(',')))\n",
        "\n",
        "    num_filters = hp.Choice('num_filters', [64,128,256])\n",
        "\n",
        "    input_tensor = Input(shape=(28, 28, 3))\n",
        "    network = Conv2D(filters=num_filters, kernel_size=kernel_size, strides=1, padding='same')(input_tensor)\n",
        "    network = MaxPooling2D((1,1))(network)\n",
        "    network = BatchNormalization()(network)\n",
        "    network = ReLU()(network)\n",
        "    network = GlobalAveragePooling2D()(network)\n",
        "    output_tensor = Dense(8, activation='softmax')(network)\n",
        "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='output',\n",
        "    project_name='BloodMNISTGridSearch'\n",
        ")\n",
        "\n",
        "tuner.search(x_train, y_train, epochs=20, validation_data=(x_val, y_val))\n",
        "\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "print(\"Best hyperparameters:\", best_hyperparameters.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdhBIgOuYRbN"
      },
      "source": [
        "## (c) Melhor desempenho da CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQeUYzb5jdE8"
      },
      "outputs": [],
      "source": [
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(train.labels.ravel())\n",
        "y_val = lb.fit_transform(val.labels.ravel())\n",
        "y_test = lb.transform(test.labels.ravel())\n",
        "\n",
        "input_tensor = Input(shape=(28, 28, 3))\n",
        "\n",
        "network = Conv2D(filters=256, kernel_size=2, strides=1, padding='same')(input_tensor)\n",
        "network = MaxPooling2D((1,1))(network)\n",
        "network = BatchNormalization()(network)\n",
        "network = ReLU()(network)\n",
        "\n",
        "network = GlobalAveragePooling2D()(network)\n",
        "output_tensor = Dense(8, activation='softmax')(network)\n",
        "\n",
        "model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint('best_model_cnn1.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.0001)\n",
        "\n",
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[checkpoint, early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxHtDpEnjmjN"
      },
      "source": [
        "Plote matriz de confusão e 5 instâncias erradas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E16GL32kjlWf"
      },
      "outputs": [],
      "source": [
        "#test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "#print(f'Teste de Acurácia: {test_acc:.3f}, Teste de Perda: {test_loss:.3f}')\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_indices = np.argmax(y_pred, axis=1)\n",
        "\n",
        "y_test_ori = test.labels.ravel()\n",
        "\n",
        "#cm = confusion_matrix(y_test_ori, y_pred_indices)\n",
        "#plt.figure(figsize=(8, 6))\n",
        "#sns.heatmap(cm, annot=True, cmap='Reds', fmt='g', xticklabels=['0','1', '2', '3', '4', '5', '6','7'], yticklabels=['0','1', '2', '3', '4', '5', '6','7'])\n",
        "#plt.xlabel('Predicted')\n",
        "#plt.ylabel('True')\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "errors = np.where(y_pred_indices != y_test_ori)[0]\n",
        "\n",
        "selected_errors = np.random.choice(errors, 5, replace=False)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20, 5))\n",
        "\n",
        "\n",
        "for i, error_index in enumerate(selected_errors):\n",
        "    ax_image = axes[i, 0]\n",
        "    ax_image.imshow(x_test[error_index].reshape(28, 28, 3))\n",
        "    ax_image.set_title('')\n",
        "    ax_image.axis('off')\n",
        "\n",
        "\n",
        "    ax_label = axes[i, 1]\n",
        "    true_label = y_test_ori[error_index]\n",
        "    pred_label = y_pred_indices[error_index]\n",
        "    label_text = f\"True: {lb.classes_[true_label]}\\nPredict: {lb.classes_[pred_label]}\"\n",
        "    ax_label.text(0.01, 0.5, label_text, ha='center', va='center', fontsize=12, family='monospace')\n",
        "    ax_label.axis('off')\n",
        "\n",
        "\n",
        "    ax_probs1 = axes[i, 2]\n",
        "    predicted_probs = y_pred[error_index]\n",
        "    probs_text1 = \"\\n\".join([f\"{lb.classes_[j]}: {prob * 100:.2f}%\" for j, prob in enumerate(predicted_probs[:4])])\n",
        "    ax_probs1.text(0.01, 0.5, probs_text1, ha='center', va='center', fontsize=12, family='monospace')\n",
        "    ax_probs1.axis('off')\n",
        "\n",
        "\n",
        "    ax_probs2 = axes[i, 3]\n",
        "    probs_text2 = \"\\n\".join([f\"{lb.classes_[j] + 4}: {prob * 100:.2f}%\" for j, prob in enumerate(predicted_probs[4:])])\n",
        "    ax_probs2.text(0.01, 0.5, probs_text2, ha='center', va='center', fontsize=12, family='monospace')\n",
        "    ax_probs2.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vkmhAwzYVJt"
      },
      "source": [
        "##(d) CNN profunda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "Zj4maCpu1uxh",
        "outputId": "ec95a31a-cbf2-4a09-aab8-6e001a53b70d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Epoch 1/30\n",
            "  3/374 [..............................] - ETA: 2:01:02 - loss: 4.2347 - accuracy: 0.1875"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-b4e9fea6db14>\u001b[0m in \u001b[0;36m<cell line: 91>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m#treinamento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m history = model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m     92\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from medmnist import BloodMNIST\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "#Criação do bloco residual\n",
        "def residual_block(x, filters, kernel_size=3, stride=1):\n",
        "\n",
        "    #faz uma cópia da entrada para ser somada com a saída depois\n",
        "    shortcut = x\n",
        "\n",
        "    #Arq da resnet, camada conv -> BN -> relu -> conv -> BN\n",
        "    x = Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv2D(filters, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    #torna as dimensões iguais, caso o salto seja diferente de 1 ou tenha num de filtros dierentes\n",
        "    if stride != 1 or shortcut.shape[-1] != filters:\n",
        "        shortcut = Conv2D(filters, kernel_size=1, strides=stride, padding='same')(shortcut)\n",
        "\n",
        "    #soma saída e entrada e passa na relu\n",
        "    x = Add()([x, shortcut])\n",
        "    x = ReLU()(x)\n",
        "    return x\n",
        "\n",
        "train = BloodMNIST(split=\"train\", download= True, size = 28)\n",
        "\n",
        "val = BloodMNIST(split=\"val\", download= True, size = 28)\n",
        "\n",
        "test = BloodMNIST(split=\"test\", download= True, size = 28)\n",
        "\n",
        "#entrada da rede\n",
        "input_tensor = Input(shape=(28, 28, 3))\n",
        "\n",
        "#primeira camada conv\n",
        "x = Conv2D(64, kernel_size=7, strides=2, padding='same')(input_tensor)\n",
        "x = BatchNormalization()(x)\n",
        "x = ReLU()(x)\n",
        "\n",
        "#Blocos residuais\n",
        "x = residual_block(x, 128, stride=1)\n",
        "x = residual_block(x, 128, stride=1)\n",
        "x = residual_block(x, 128, stride=1)\n",
        "x = residual_block(x, 128, stride=1)\n",
        "x = residual_block(x, 128, stride=1)\n",
        "x = residual_block(x, 256, stride=1)\n",
        "x = residual_block(x, 256, stride=1)\n",
        "x = residual_block(x, 256, stride=1)\n",
        "x = residual_block(x, 256, stride=1)\n",
        "x = residual_block(x, 256, stride=1)\n",
        "x = residual_block(x, 128, stride=1)\n",
        "x = residual_block(x, 128, stride=1)\n",
        "x = residual_block(x, 128, stride=1)\n",
        "x = residual_block(x, 128, stride=1)\n",
        "x = residual_block(x, 128, stride=1)\n",
        "\n",
        "#camada de pooling e saída\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "output_tensor = Dense(8, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#model.summary()\n",
        "\n",
        "#normalização da entrada (entre 0 e 1)\n",
        "x_train = train.imgs / 255\n",
        "x_val = val.imgs / 255\n",
        "x_test = test.imgs / 255\n",
        "\n",
        "#torna as labels no formato de vetor binário\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(train.labels.ravel())\n",
        "y_val = lb.fit_transform(val.labels.ravel())\n",
        "y_test = lb.transform(test.labels.ravel())\n",
        "\n",
        "#callback pra early_stopping e para salvar melhor modelo com base em val\n",
        "checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=8, min_delta=0.0001)\n",
        "\n",
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "#treinamento\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[checkpoint, early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxn97ctH2BJY"
      },
      "outputs": [],
      "source": [
        "model = load_model('best_model.keras')\n",
        "\n",
        "#avaliar o modelo no conjunto de testes\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Teste de Acurácia: {test_acc:.3f}, Teste de Perda: {test_loss:.3f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
